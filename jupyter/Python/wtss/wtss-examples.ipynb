{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/wtss.png?raw=true\" align=\"right\" width=\"64\"/>\n",
    "\n",
    "# <span style=\"color:#336699\">Web Time Series Service (WTSS) - Examples</span>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "    <a href=\"https://nbviewer.jupyter.org/github/brazil-data-cube/code-gallery/blob/master/jupyter/Python/wtss/wtss-examples.ipynb\"><img src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" align=\"center\"/></a>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Rennan F. B. Marujo<sup><a href=\"https://orcid.org/0000-0002-0082-9498\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Matheus Zaglia<sup><a href=\"https://orcid.org/0000-0001-6181-2158\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Gilberto R. Queiroz<sup><a href=\"https://orcid.org/0000-0001-7534-0219\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Felipe Menino Carlos<sup><a href=\"https://orcid.org/0000-0002-3334-4315\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    Earth Observation and Geoinformatics Division, National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:brazildatacube@inpe.br\">brazildatacube@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: March 12, 2021\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter Notebook gives shows how to use the WTSS service to extract time series from <em>Brazil Data Cube</em>' service and how to perform a basic time series manipulation.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>This Jupyter Notebook is a supplement to the following paper:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px\">\n",
    "    Vinhas, L.; Queiroz, G. R.; Ferreira, K. R.; Camara, G. <a href=\"http://www.seer.ufu.br/index.php/revistabrasileiracartografia/article/view/44004\" target=\"_blank\">Web Services for Big Earth Observation Data</a>. Revista Brasileira de Cartografia, v. 69, n. 5, 18 maio 2017.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Client API\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the examples that will be presented in this Jupyter Notebook, you will need to have installed the Python library [wtss.py](https://github.com/brazil-data-cube/wtss.py). Additionally, for the time series visualization and interpolation examples, you will need to have installed the libraries [NumPy](https://numpy.org/), [Pandas](https://pandas.pydata.org/), [Matplotlib](https://matplotlib.org/), [Plotly](https://plotly.com/python/), [SciPy](https://www.scipy.org/) and [Folium] (http://python-visualization.github.io/folium/).\n",
    "\n",
    "The installation of the mentioned libraries can be done with the service [Python Package Index (PyPI)](https://pypi.org/), through the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install wtss[matplotlib] numpy pandas matplotlib plotly scipy folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information on [WTSS client for Python](https://github.com/brazil-data-cube/wtss.py), see the introductory Jupyter Notebook about [Web Time Series Service (WTSS)](./wtss-introduction.ipynb) Introduction notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the service and Search for time series\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the WTSS client library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from wtss import WTSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the service to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = WTSS('https://brazildatacube.dpi.inpe.br/', access_token='change-me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.coverages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access the CBERS-4/AWFI coverage using the `CB4_64_16D_STK-1` product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbers4_coverage = service['CB4_64_16D_STK-1']\n",
    "cbers4_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBERS-4/AWFI spectral bands 15 and 16 correspond to the red and near-infrared (NIR) wavelength regions, respectivelly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_band = 'BAND15'\n",
    "nir_band = 'BAND16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the time series for data product `CB4_64_16D_STK-1`, in the location of `latitude -16.817` and `longitude -52.079` from January 1st, 2017 to December 31st, 2019, using the `ts` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = cbers4_coverage.ts(attributes=(red_band, nir_band),\n",
    "                                 latitude=-16.817,\n",
    "                                 longitude=-52.079,\n",
    "                                 start_date=\"2017-01-01\",\n",
    "                                 end_date=\"2019-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Time Series\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Static Visualization**\n",
    "\n",
    "Once with the time series, one can visualize it using the `plot` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive Visualization**\n",
    "\n",
    "Another way to visualize the time series is through interactive graphics, which allow dinamically explore the time series patterns.\n",
    "\n",
    "To do that, we will use the [Plotly](https://plotly.com/python/) and [Pandas](https://pandas.pydata.org/) libraries, which can be imported as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `pandas.DataFrame` containing the temporal attributes for `BAND15` and `BAND16`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbers_df = pd.DataFrame({ 'BAND15': time_series.BAND15, 'BAND16': time_series.BAND16 }, \n",
    "                        index = pd.to_datetime(time_series.timeline))\n",
    "\n",
    "cbers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `pandas.DataFrame` to create an interactive graphic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(cbers_df, x=cbers_df.index, y=['BAND15', 'BAND16'], title='CBERS-4/AWFI (BAND 15 and 16)', labels={\n",
    "    'index': 'Date',\n",
    "    'value': 'Spectral Reflectance (scaled)'\n",
    "})\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing clouds from Time Series\n",
    "<hr style=\"border:1px solid #0077b9;\">\n",
    "\n",
    "When we work with satellite image time series there are external influences that can bring noise to the values of the series, distorting them. A common example of noise is the observation of clouds, which change the spectral behavior of targets, making them often unidentifiable. In Figure 1, images from the Sentinel-2/MSI sensor satellite of the `20LKP` tile are shown, where it is possible to observe that, for the same region, in close dates, the presence of clouds.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <figcaption><strong>Figura 1</strong> - Imagens Sentinel-2/MSI (Tile 20LKP) com influência de Nuvem</figcaption>\n",
    "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/sentinel-2-clouds.png?raw=true\" align=\"center\" width=\"620rem\"/>\n",
    "    <br>\n",
    "    <strong>Fonte</strong>: Simoes <i>et al.</i> (2021)\n",
    "</div>\n",
    "\n",
    "In the context of satellite image time series, a possible approach for the treatment of this type of noise is to replace the time series points that have cloud influence by interpolated values.\n",
    "\n",
    "For this, it is necessary to have data available to identify the influence of the cloud at each point in the time series. When considering the BDC data products, this approach is possible since the data cubes are produced with masks that identify, *pixel* to *pixel*, the cloud influence on the images.\n",
    "\n",
    "Therefore, in this second example, we will use the cloud mask provided in the BDC data products, together with the linear interpolation technique, to remove the cloud influence from the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, first lets look at the `CB4_64_16D_STK-1` *Coverage* metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbers4_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this metadata, we can see the attribute `CMASK`, which stands for cloud mask and uses the following values:\n",
    "\n",
    "- `0`: *Pixel* no data;\n",
    "- `127`: *Pixel* clear observation (no cloud);\n",
    "- `255`: *Pixel* cloud observation.\n",
    "\n",
    "Let's retrieve the time series, for the location `latitude -12.0`, `longitude -53.989` from `January 1, 2017` to `December 31, 2019`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb4_timeseries_cmask = cbers4_coverage.ts(\n",
    "\n",
    "    attributes = ('CMASK'),\n",
    "\n",
    "    latitude = -12.0,\n",
    "    longitude = -53.989,\n",
    "\n",
    "    start_date = \"2017-01-01\",\n",
    "    end_date = \"2019-12-31\"\n",
    ")\n",
    "\n",
    "cb4_timeseries_cmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the returned values. To make the visualization simpler, let's remove the values that are duplicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(cb4_timeseries_cmask.values('CMASK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, among the values returned, there are *pixels* that have values `127` and `255`. This indicates that certain values have cloud (`255`) and may be removed.\n",
    "\n",
    "To present the example of interpolation, let's make use of the `NDVI` attribute. So, first let's do a new time series extraction, considering now both attributes, `NDVI` and `CMASK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb4_timeseries = cbers4_coverage.ts(\n",
    "\n",
    "    attributes = ('NDVI', 'CMASK'),\n",
    "\n",
    "    latitude = -12.0,\n",
    "    longitude = -53.989,\n",
    "\n",
    "    start_date = \"2017-01-01\",\n",
    "    end_date = \"2019-12-31\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform this operation step by step. So, first let's retrieve the timeline from the extracted time series and then transform them into `datetime` type objects. This can be done with the command below:\n",
    "\n",
    "<div style=\"text-align: center; margin-left: 25%; margin-right: 25%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>Tip:</b> The timeline to <em>datetime</em> conversion operation is necessary since, although the <em>cb4_timeseries.timeline</em> attribute stores a list of dates, in Python, the values in this list are just text and are not interpreted by the language as valid dates. This can make it difficult to manipulate this data, especially when considering aggregations and temporal selections of the data using the <em>Pandas</em> library.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_timeline = pd.to_datetime(cb4_timeseries.timeline)\n",
    "ndvi_timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's store the values of the `NDVI` and `CMASK` attributes of the extracted time series in a `numpy.array`. This will be done to facilitate the manipulation of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ndvi_data = np.array(cb4_timeseries.NDVI)\n",
    "ndvi_data\n",
    "\n",
    "cmask_data = np.array(cb4_timeseries.CMASK)\n",
    "cmask_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to adopt some strategy to make the values that have cloud influence be marked as `nan` in the dataset.\n",
    "\n",
    "As the values of both attributes are stored in a `numpy.array`, we can then use the features of [numpy](https://numpy.org/), to manipulate this data.\n",
    "\n",
    "Considering this, we are going to adopt a strategy in which we will apply a transformation to the `CMASK` data, so that every position in the array that represents cloud influence will be converted to `nan`, while the other values will be converted to `1`. With this approach, we will be able to multiply the array resulting from the transformation, with the data of other attributes, which will make the positions with cloud influence become `nan` as well.\n",
    "\n",
    "To be clear, let's perform this operation. First, with the code below, let's convert the values of `CMASK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmask_data = np.where(cmask_data == 255, np.nan, 1)\n",
    "cmask_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now, there are some `nan` values in the `cmask` array, while all others are equal to `1`. By multiplying this mask array, with the `NDVI` array, we have the following effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_data * cmask_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that among the values of `NDVI` in the time series, some values were transformed into `nan`. Now, we can perform the interpolation of our time series.\n",
    "\n",
    "For that, we're going to put this data inside a `pandas.DataFrame`. As we only have one data attribute, `NDVI`, let's multiply it with the mask array and store it in a column of the `DataFrame`, named `data`. The index (Name of the lines of the `DataFrame`), will be each of the dates of the timeline (Converted to datetime in the previous step).\n",
    "\n",
    "The command below creates the `DataFrame` with the specified characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_masked_data = pd.DataFrame({ 'data': ndvi_data * cmask_data }, index = pd.to_datetime(ndvi_timeline))\n",
    "ndvi_masked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look in the `DataFrame` for the lines that have `na` values, we should find some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_masked_data[ndvi_masked_data['data'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `interpolate` method, available in `pandas.DataFrame`, let's interpolate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_masked_data_interpolated = ndvi_masked_data.interpolate()\n",
    "ndvi_masked_data_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolation result was saved in the `ndvi_masked_data_interpolated` variable. When querying this variable for the existence of `nan` values, no value should be returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_masked_data_interpolated[ndvi_masked_data_interpolated['data'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We interpolate our data. To visualize the result of this operation and the difference caused by this change in the series, below, let's visualize the original series and the interpolated series.\n",
    "\n",
    "As the [Matplotlib](https://matplotlib.org/) library will be used in this view, it must be imported. This can be done with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "\n",
    "plt.plot(ndvi_data, color='gray', linestyle='dashed', label = 'Original')\n",
    "plt.plot(ndvi_masked_data_interpolated['data'].values, color='blue', label = 'Interpolated')\n",
    "\n",
    "plt.title('Comparison of Time Series with and without interpolation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series smoothing\n",
    "<hr style=\"border:1px solid #0077b9;\">\n",
    "\n",
    "In addition to cloud noise, there are many other factors that can influence the time series values ​​of Remote Sensing images. For these cases, a solution can be to apply filters to smooth the time series.\n",
    "\n",
    "There are several smoothing algorithms that can be used, in this example the smoothing of the time series will be presented using the `Savitzky Golay` filter. The time series used in this example will be the same interpolated in the previous step.\n",
    "\n",
    "For the example, the `Savitzky Golay` filter implemented in the library [SciPy](https://www.scipy.org/) will be used. Importing the filter function can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filter function imported, let's apply it to the interpolated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_smoothed = savgol_filter(ndvi_masked_data_interpolated['data'], window_length = 9, polyorder = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-left: 25%; margin-right: 25%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>Note:</b> The <em>Savitz Golay</em> algorithm uses <b>window_length</b> and the <b>polynomial order</b> used as parameters. You can change these values ​​to see the impacts on the smoothing of the series. Additionally, if you are more interested in time series smoothing, it is recommended to read <a href=\"https://e-sensing.github.io/sitsbook/working-with-time-series.html#filtering -techniques-for-time-series\" target=\"_blank\"> \"Section 3.5 - Filtering techniques for time series\" of the <em>sits</em></a> user manual\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the results, in the same way as for the interpolation, let's create a plot that allows us to visualize the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "\n",
    "plt.plot(ndvi_masked_data_interpolated['data'].values, color='gray', linestyle='dashed', label = 'Interpolated')\n",
    "plt.plot(median_smoothed, color='blue', label = 'Interpolated and smoothed')\n",
    "\n",
    "plt.title('Comparison of Time Series with and without interpolation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Median Time Series\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all examples have made use of time series varying only in the number of attributes. Thus, in this example, a set of time series will be used in order to extract a median time series.\n",
    "\n",
    "The first thing to do then, is to determine the points that will be used. In this case, for the extraction of time series, we will generate points in an agricultural region in the state of Mato Grosso. These points will be generated considering the following criteria:\n",
    "\n",
    "- Longitude: Fixed at the value `-53,989`;\n",
    "- Latitude: Variable from `-16.9000` to `-16.9075` at a rate of `-0.0015`.\n",
    "\n",
    "This will generate five points. These will be used to extract time series from the previously selected *Coverage*, considering data from `January 1, 2017` to `December 31, 2019`. Only the NDVI vegetation index attribute will be used.\n",
    "\n",
    "Let's start with generating the points, which can be done with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = -53.989  # Fixado\n",
    "locations = []  # variável\n",
    "\n",
    "for latitude in np.arange(-16.9000,-16.9075,-0.0015):\n",
    "    locations.append(( latitude, longitude ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the operation, we have a set of locations stored in the `locations` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To more clearly present the region from which the time series will be collected, let's place the generated points on an interactive map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    location = [-16.9, -53.989],\n",
    "    zoom_start = 15,\n",
    ")\n",
    "\n",
    "tile = folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add the points to the map and then do the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in locations:\n",
    "    folium.Circle(\n",
    "        fill=True,  # Preenchimento\n",
    "        color = 'orange',  # Cor\n",
    "        location=location,  # Localização\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the time series from each of these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture_time_series = []\n",
    "\n",
    "for location in locations:\n",
    "    latitude, longitude = location\n",
    "\n",
    "    time_series = cbers4_coverage.ts(attributes=('NDVI'),\n",
    "                                     latitude=float(latitude), longitude=float(longitude),\n",
    "                                     start_date=\"2017-01-01\", end_date=\"2019-12-31\")\n",
    "\n",
    "    agriculture_time_series.append(time_series.values('NDVI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had 6 points, we must have saved 6 time series. Let's check it out by looking at the number of elements in the `agriculture_time_series` list. This can be done with the `len` function, as shown in the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agriculture_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `median` function from the [NumPy](https://numpy.org/) library, let's calculate the median value for each point in the time series:\n",
    "\n",
    "<div style=\"text-align: center; margin-left: 25%; margin-right: 25%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>Tip:</b> The operation <em>median</em> will use the parameter <em>axis=0</em> so that the median is calculated considering a set of points in each instant of time, so that the median of each instant of time is the median value of 5 values ​​(One of each point).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(agriculture_time_series, axis = 0)\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize each of the original time series and compare them to the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "\n",
    "#\n",
    "# Criando plot das séries temporais originais\n",
    "#\n",
    "for i in range(len(agriculture_time_series)):\n",
    "    plt.plot(agriculture_time_series[i], color = 'grey', alpha = .2)\n",
    "\n",
    "plt.plot(median, color='blue', linewidth = 1)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:1px solid #0077b9;\">\n",
    "\n",
    "- [Python Client Library for Web Time Series Service - User Guide](https://wtss.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "- [Python Client Library for Web Time Series Service - GitHub Repository](https://github.com/brazil-data-cube/wtss.py)\n",
    "\n",
    "\n",
    "- [WTSS OpenAPI 3 Specification](https://github.com/brazil-data-cube/wtss-spec)\n",
    "\n",
    "\n",
    "- VINHAS, L.; QUEIROZ, G. R.; FERREIRA, K. R.; CÂMARA, G. [Web Services for Big Earth Observation Data](http://www.seer.ufu.br/index.php/revistabrasileiracartografia/article/view/44004). Revista Brasileira de Cartografia, v. 69, n. 5, 18 maio 2017.\n",
    "\n",
    "- Simoes, R., Camara, G., Souza, F., Santos, L., Andrade, P. R., Peletier, C., Carvalho, A., Ferreira, K., Queiroz, G., & Maus, V. (2021). sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series. Online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See also the following Jupyter Notebooks\n",
    "<hr style=\"border:1px solid #0077b9;\">\n",
    "\n",
    "* [Introduction to the Web Time Series Service (WTSS)](./wtss-introduction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
