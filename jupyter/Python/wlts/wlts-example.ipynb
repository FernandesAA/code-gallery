{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacterial-aquarium",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/logo-bdc.png\" align=\"right\" width=\"64\"/>\n",
    "\n",
    "# <span style=\"color:#336699\">Labelling data points with WLTS</span>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "<div style=text-align: left;>\n",
    "    <a href=\"https://nbviewer.jupyter.org/github/brazil-data-cube/code-gallery/blob/master/jupyter/Python/wlts/wlts-example.ipynb\"><img src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" align=\"center\"/></a>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Fabiana Zioti<sup><a href=\"https://orcid.org/0000-0002-7305-6043\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Felipe Menino Carlos<sup><a href=\"https://orcid.org/0000-0002-3334-4315\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Karine Reis Ferreira<sup><a href=\"https://orcid.org/0000-0003-2656-5504\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Gilberto R. Queiroz<sup><a href=\"https://orcid.org/0000-0001-7534-0219\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    Earth Observation and Geoinformatics Division, National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:brazildatacube@inpe.br\">brazildatacube@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: March 24, 2021\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter Notebook shows how to use the WLTS API to label a set of data points considering two well known land use and land cover collections: (1) Projeto de Mapeamento Anual da Cobertura e Uso do Solo no Brasil (MapBiomas) version 5; (2) Mapa de uso e cobertura da Terra and Instituto Brasileiro de Geografia e Estatística (IBGE) - Monitoramento e uso da Terra. The data points were created using a regular grid. After labelling these data points, the example illustrates a possible scenario for comparing the class agreement between the collections regarding these points. Finally, the data points with agreement in both collections are used to compute a water mask over an region of interest.\n",
    "</div>    \n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>This Jupyter Notebook is supplement to the following papers:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px; margin-top:10px\">\n",
    "      <p> Ferreira, K.R.; Queiroz, G.R.; Vinhas, L.; Marujo, R.F.B.; Simoes, R.E.O.; Picoli, M.C.A.; Camara, G.; Cartaxo, R.; Gomes, V.C.F.; Santos, L.A.; Sanchez, A.H.; Arcanjo, J.S.; Fronza, J.G.; Noronha, C.A.; Costa, R.W.; Zaglia, M.C.; Zioti, F.; Korting, T.S.; Soares, A.R.; Chaves, M.E.D.; Fonseca, L.M.G. 2020. Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sens. 12, no. 24: 4033. DOI: <a href=\"https://doi.org/10.3390/rs12244033\" target=\"_blank\">10.3390/rs12244033</a>. </p>\n",
    "      <p> Zioti, F.; Gomes, V.C.F.; Ferreira, K.R.; Queiroz, G.R.; Rodriguez, E. L. 2019. Um ambiente para acesso e análise de trajetórias de uso e cobertura da Terra. Anais do XIX Simpósio Brasileiro de Sensoriamento Remoto.São José dos Campos, INPE, 2019. <a href=\"https://proceedings.science/sbsr-2019/papers/um-ambiente-para-acesso-e-analise-de-trajetorias-de-uso-e-cobertura-da-terra\" target=\"_blank\"> Online </a>. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-trailer",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<hr style=\"border:1px solid #0077b9;\">\n",
    "\n",
    "This notebook contains a simplified example of applying WLTS in the context of sample labeling. The service is used for label extraction considering a set of geometries with locations in geographic space. This allows for each of the points being used to be associated with a class. Such activity can assist tasks of temporal analysis of land use and land cover, classifications, and several other applications.\n",
    "\n",
    "In this example, two classification projects are considered during the extraction to be used for a concordance analysis. In this way, the labels used consider the knowledge applied in each of the projects.\n",
    "\n",
    "Thus, as summarized by the Figure below, in this notebook, we will perform four main steps:\n",
    "\n",
    "1. **Selection of the collections**\n",
    "2. **Definition of a specific time instant**\n",
    "3. **Extraction of the labels**\n",
    "4. **Analysis of concordance**\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../../img/wlts/overview_example.png\" width=\"640\" />\n",
    "    <br/>\n",
    "    <b>Figure 1</b> - notebook steps\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-break",
   "metadata": {},
   "source": [
    "## Study Area\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-metallic",
   "metadata": {},
   "source": [
    "As depicted in Figure 1, the study area is located in the Pará state, Brazil, in the Amazon biome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-mountain",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/example_wlts_area.png\" width=\"600\" />,\n",
    "    <br/>\n",
    "    <b>Figure 2</b> - Labelling data points with WLTS- Study Area.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-devon",
   "metadata": {},
   "source": [
    "## Python Client API\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-savannah",
   "metadata": {},
   "source": [
    "For running the examples in this Jupyter Notebook you will need to install the [WLTS client for Python](https://github.com/brazil-data-cube/wlts.py).To install it from GitHub using `pip`, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/brazil-data-cube/wlts.py@v0.4.0-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-implement",
   "metadata": {},
   "source": [
    "We also use the follow libraries: [numpy](https://numpy.org/), [rasterio](https://rasterio.readthedocs.io/en/latest/), [pandas](https://pandas.pydata.org/), [geopandas](https://geopandas.org/), [seaborn](https://seaborn.pydata.org/), [matplotlib](https://matplotlib.org/), [sklearn](https://scikit-learn.org/stable/), [stac.py](https://github.com/brazil-data-cube/stac.py). To install those libraries from PyPI using pip, use the following commands:\n",
    "\n",
    "> pip install numpy rasterio pandas geopandas seaborn matplotlib sklearn folium stac.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-pendant",
   "metadata": {},
   "source": [
    "## Set the service and load the data points\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wlts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-alloy",
   "metadata": {},
   "source": [
    "Define the service to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = wlts.WLTS('https://brazildatacube.dpi.inpe.br/wlts/')\n",
    "service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "service.collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-salmon",
   "metadata": {},
   "source": [
    "**Sampling GRID**\n",
    "\n",
    "We have prepared the grid shown in Figure 1 as an ESRI Shapefile represented with point geometries. The next cell uses GeoPandas to load this file from a given URL:\n",
    "\n",
    ">  The sample points used below were generated using [QGIS GIS](https://qgis.org/pt_BR/site/). If you wish, you can use the [Verde] library (https://www.fatiando.org/verde/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = geopandas.read_file(\"/vsicurl/https://brazildatacube.dpi.inpe.br/\" \\\n",
    "                                 \"public/workshop/bdc-2020-03/wlts/samples-points/10x10/\" \\\n",
    "                                 \"sample-points.shp\")\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-invitation",
   "metadata": {},
   "source": [
    "Below, each grid point's spatial location is presented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# extract sample long, lat\n",
    "#\n",
    "latlon = samples_df.geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "\n",
    "#\n",
    "# create folium map\n",
    "#\n",
    "folium_map = folium.Map( location=[-0.52, -51.1526], zoom_start=12)\n",
    "\n",
    "#\n",
    "# Google Satellite Layer\n",
    "#\n",
    "tile = folium.TileLayer(\n",
    "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
    "        attr = 'Google',\n",
    "        name = 'Google Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(folium_map)\n",
    "\n",
    "#\n",
    "# add marker to map\n",
    "#\n",
    "for coord in latlon:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], fill_color='#43d9de', radius=3).add_to( folium_map )\n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-precipitation",
   "metadata": {},
   "source": [
    "## Data Collections\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-antenna",
   "metadata": {},
   "source": [
    "**MapBiomas version 5 - Mapa de uso e cobertura da Terra**\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/mapbiomas-5v2.png\" width=\"600\" />,\n",
    "    <br/>\n",
    "    <b>Figure 3</b> - MapBiomas version 5.\n",
    "</center>\n",
    "\n",
    "\n",
    "    \n",
    "**IBGE - Monitoramento e uso da Terra (2018)**\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/mapa-ibge.png\" width=\"600\" />,\n",
    "    <br/>\n",
    "    <b>Figure 4</b> - IBGE - Monitoramento e uso da Terra (2018).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-gambling",
   "metadata": {},
   "source": [
    "# Labelling the data points\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-unemployment",
   "metadata": {},
   "source": [
    "For each point in the grid we search for a feature in both projects and if found we assign the feature's label to the data point.\n",
    "\n",
    "In WLTS, each project dataset is represented as a `collection`. Thus, a `collection` is an aggregation of data from different years of the same project.\n",
    "\n",
    "In this example scenario, we will perform a concordance analysis between each project to choose the best samples. This analysis provides a set of samples based on the knowledge applied by each project for the choice of classes.\n",
    "\n",
    "> Note that this is an example scenario. The complexities of a real scenario are not considered here. Therefore, problems related to Spatio-temporal differences between samples or their creation methodology are not considered!\n",
    "\n",
    "The sample labels will be extracted separately in the subsections to facilitate their application in the example that will be created, but the [wlts.py library](https://github.com/brazil-data-cube/wlts.py/) supports the extraction of data considering multiple projects at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-consent",
   "metadata": {},
   "source": [
    "**IBGE - Monitoramento e uso da Terra (2018)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-provincial",
   "metadata": {},
   "source": [
    "In WLTS, the collection with IBGE data from the Land Use Monitoring project is in the collection named `ibge_land_use_cover`. The code below extracts the label of this collection in the year 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge = []\n",
    "\n",
    "#\n",
    "# Extract classes with WLTS\n",
    "#\n",
    "for point_row in samples_df.iterrows():\n",
    "    point_row = point_row[1]\n",
    "    \n",
    "    ibge_class = service.tj(latitude  = point_row.geometry.y, \n",
    "                            longitude = point_row.geometry.x, \n",
    "                            start_date = 2018, end_date = 2018,\n",
    "                            collections = \"ibge_cobertura_uso_terra\")\n",
    "    \n",
    "    samples_ibge.append(ibge_class.df())\n",
    "\n",
    "#\n",
    "# Create a Data Frame\n",
    "#\n",
    "samples_ibge = pandas.concat(samples_ibge).reset_index(drop=True)\n",
    "samples_ibge[\"geometry\"] = samples_df[\"geometry\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-kingdom",
   "metadata": {},
   "source": [
    "The table below presents the classes, with only one year, extracted for all the grid points presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-object",
   "metadata": {},
   "source": [
    "**MapBiomas version 5 - Mapa de uso e cobertura da Terra**\n",
    "\n",
    "Analogous to the IBGE data, this section extracts the data from MapBiomas. In WLTS, the data from MapBiomas (Version 5) are represented through the collection `mapbiomas5_amazonia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas = []\n",
    "\n",
    "#\n",
    "# Extract classes with WLTS\n",
    "#\n",
    "for point_row in samples_df.iterrows():\n",
    "    point_row = point_row[1]\n",
    "    \n",
    "    mapbiomas_class = service.tj(latitude  = point_row.geometry.y, \n",
    "                                 longitude = point_row.geometry.x, \n",
    "                                 start_date = 2018,\n",
    "                                 end_date = 2018,\n",
    "                                 collections = \"mapbiomas5_amazonia\")\n",
    "    \n",
    "    samples_mapbiomas.append(mapbiomas_class.df())\n",
    "\n",
    "#\n",
    "# Create a Data Frame\n",
    "#\n",
    "samples_mapbiomas = pandas.concat(samples_mapbiomas).reset_index(drop=True)\n",
    "samples_mapbiomas[\"geometry\"] = samples_df[\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-lingerie",
   "metadata": {},
   "source": [
    "# Prepare data to a concordance analysis\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-amendment",
   "metadata": {},
   "source": [
    "This section prepares the data for the concordance analysis. In this process, all points identified as water have their path values converted to `1`, while all other values are represented by `0`.\n",
    "\n",
    "This conversion is applied considering that there is one class that represents the Water element for each collection. The table below summarizes how each collection does this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-occasions",
   "metadata": {},
   "source": [
    "|         Collection        \t|      Nomenclature for water class   \t|\n",
    "|:-------------------------:\t|:----------------------------------:\t|\n",
    "|        IBGE (2018)        \t|      Corpo d'água Continental      \t|\n",
    "| MapBiomas Versão 5 (2018) \t|         Rio, Lago e Oceano         \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-specific",
   "metadata": {},
   "source": [
    "Considering the information in the table, below each of the collections is prepared for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-turkey",
   "metadata": {},
   "source": [
    "`IBGE Collection (2018)`\n",
    "\n",
    "> After running the command below, notice that the `class` column has its value summed to the values `0` and `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.loc[samples_ibge[\"class\"] != \"Corpo d'água Continental\", \"class\"] = 0\n",
    "samples_ibge.loc[samples_ibge[\"class\"] == \"Corpo d'água Continental\", \"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-alpha",
   "metadata": {},
   "source": [
    "`MaBiomas Collection (2018)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] != \"Rio, Lago e Oceano\", \"class\"] = 0\n",
    "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] == \"Rio, Lago e Oceano\", \"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-sensitivity",
   "metadata": {},
   "source": [
    "# Concordance analysis\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-compression",
   "metadata": {},
   "source": [
    "Below we will generate an example of a concordance analysis. A confusion matrix is generated to visualize and quantify the points that have a concordance. After visualizing the matrix, the data is filtered so that only the points where there is concordance are considered.\n",
    "\n",
    "> **Note**: The analysis below does not consider many of the practical complexities involved in this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate the confusion matrix\n",
    "#\n",
    "\n",
    "cm_arr = confusion_matrix(samples_ibge[\"class\"].astype(\"int\"), samples_mapbiomas[\"class\"].astype(\"int\"))\n",
    "\n",
    "#\n",
    "# formating results\n",
    "#\n",
    "reference = [\"Non-Water\", \"Water\"]\n",
    "\n",
    "cm_pd = pandas.DataFrame(cm_arr, columns = reference, index = reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 100)\n",
    "\n",
    "#\n",
    "# plot matrix\n",
    "#\n",
    "seaborn.heatmap(cm_pd, annot=True, fmt = 'g', cmap=\"YlGnBu\", cbar = False)\n",
    "\n",
    "#\n",
    "# configure labels\n",
    "#\n",
    "plt.title(\"Concordance matrix\")\n",
    "plt.ylabel(\"IBGE (2018)\")\n",
    "plt.xlabel(\"MapBiomas 5 (2018)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-anderson",
   "metadata": {},
   "source": [
    "> Below, the samples are filtered considering the equality between both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate the \"true matrix\" based on classes matching\n",
    "#\n",
    "true_matrix = samples_ibge[\"class\"].values == samples_mapbiomas[\"class\"].values\n",
    "\n",
    "#\n",
    "# filtering data\n",
    "#\n",
    "samples_ibge_filtered = samples_ibge[true_matrix]\n",
    "samples_mapbiomas_filtered  = samples_mapbiomas[true_matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-edward",
   "metadata": {},
   "source": [
    "**Visualizing the filtered points in the geographical space**\n",
    "\n",
    "The map below shows the filtered samples. The `blue` samples represent the concordant elements. On the other hand, the `yellow` ones are the points where the ready did not agree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# extract sample long, lat\n",
    "#\n",
    "latlon = samples_ibge_filtered.geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "latlon_not_valid = samples_ibge[~true_matrix].geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "\n",
    "#\n",
    "# create folium map\n",
    "#\n",
    "folium_map = folium.Map( location=[-0.52, -51.1526], zoom_start=12)\n",
    "\n",
    "#\n",
    "# Google Satellite Layer\n",
    "#\n",
    "tile = folium.TileLayer(\n",
    "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
    "        attr = 'Google',\n",
    "        name = 'Google Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(folium_map)\n",
    "\n",
    "#\n",
    "# add marker to map (concordance samples)\n",
    "#\n",
    "for coord in latlon:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], fill_color='#43d9de', radius=9).add_to( folium_map )\n",
    "\n",
    "#\n",
    "# add marker to map (not concordante samples)\n",
    "#\n",
    "for coord in latlon_not_valid:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], color='#F5AD46', radius=9).add_to( folium_map )\n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-lesbian",
   "metadata": {},
   "source": [
    "# Example: Computing Water Mask for a single image\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-awareness",
   "metadata": {},
   "source": [
    "In this section, the previously extracted and filtered samples will be used for training a linear classifier. After training, the model is applied. The classification process will be done considering a scene extracted from the Landsat-8/OLI data cube (temporal composition of 16 days and the pixel choice with less cloud influence done through the STACK algorithm).\n",
    "\n",
    "The defined study region is located within the Amazon biome, in a cube tile in Pará.\n",
    "\n",
    "> In this example, to reduce the computational requirements, a small region of the scene will be used, this one intersecting with the location of the grid points presented earlier. Furthermore, to facilitate classification, the **N**ormalized **D**ifference **W**ater **I**ndex (NDWI) is calculated.\n",
    "\n",
    "For the acquisition of the Landsat-8/OLI scene the [stac.py](https://github.com/brazil-data-cube/stac.py) client will be used. Below, the client is imported and the credentials are set.\n",
    "\n",
    "> To install the [stac.py](https://github.com/brazil-data-cube/stac.py), you can use the following command:\n",
    "\n",
    "```shell\n",
    "pip install stac.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = stac.STAC('https://brazildatacube.dpi.inpe.br/stac/', access_token='change-me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# defining roi bbox\n",
    "#\n",
    "bbox = (-51.232048, \n",
    "        -0.594217, \n",
    "        -51.078365, \n",
    "        -0.464596)\n",
    "\n",
    "#\n",
    "# query stac!\n",
    "#\n",
    "items = service.search({\n",
    "    'limit': 1,\n",
    "    'bbox': bbox,\n",
    "    'datetime': '2018-06-10/2018-06-25',\n",
    "    'collections': ['LC8_30_16D_STK-1']\n",
    "})\n",
    "\n",
    "#\n",
    "# visualizing the result (must be have scene)\n",
    "#\n",
    "items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# select item\n",
    "#\n",
    "item = items.features[0]\n",
    "\n",
    "#\n",
    "# load bands to generate the rgb\n",
    "#\n",
    "band7 = item.read(\"band7\", bbox = bbox)\n",
    "\n",
    "band5 = item.read(\"band5\", bbox = bbox)\n",
    "\n",
    "band4 = item.read(\"band4\", bbox = bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-sperm",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>Note:</b> If there are errors because of your pyproj version, you can run the code below as specified in <a  href=\"https://rasterio.readthedocs.io/en/latest/faq.html#why-can-t-rasterio-find-proj-db-rasterio-from-pypi-versions-1-2-0\" target=\"_blank\">rasterio documentation</a> and try again:\n",
    "\n",
    "       import os\n",
    "       del os.environ['PROJ_LIB']\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-rebate",
   "metadata": {},
   "source": [
    "The code above, shows the scene loaded in grid region. In this example, the RGB visualization uses the `band 7`, `band 5` and `band 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# defining the rgb matrix\n",
    "#\n",
    "rgb = numpy.stack((band7, band5, band4))\n",
    "\n",
    "#\n",
    "# create a figure\n",
    "#\n",
    "plt.figure(dpi = 120)\n",
    "\n",
    "#\n",
    "# plot!\n",
    "#\n",
    "show(rgb / 10000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-missouri",
   "metadata": {},
   "source": [
    "**Calculating the Normalized Difference Water Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-broadcast",
   "metadata": {},
   "source": [
    "Loading images using [rasterio](https://rasterio.readthedocs.io/en/latest/). To do this, the url of each scene, returned in stac query is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "band3ds = rasterio.open(item[\"assets\"][\"band3\"][\"href\"])\n",
    "band5ds = rasterio.open(item[\"assets\"][\"band5\"][\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-appreciation",
   "metadata": {},
   "source": [
    "To extract values, we need reproject grid data point to raster **C**oordinate **R**eference **S**ystem (CRS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# reprojects grid points\n",
    "#\n",
    "samples_ibge_filtered = geopandas.GeoDataFrame(samples_ibge_filtered)\\\n",
    "                                .set_geometry(\"geometry\")\\\n",
    "                                .set_crs(\"EPSG:4326\")\n",
    "\n",
    "points = samples_ibge_filtered[\"geometry\"].to_crs(band3ds.crs)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-motion",
   "metadata": {},
   "source": [
    "now, we can extract spectral data (Bands 3 and 5) for each data points filterd above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extract values for band 3\n",
    "#\n",
    "band3_values = numpy.array(list(\n",
    "    band3ds.sample((x, y) for x, y in zip(points.x, points.y))\n",
    "))  / 10000\n",
    "\n",
    "#\n",
    "# Extract values for band 5\n",
    "#\n",
    "band5_values = numpy.array(list(\n",
    "    band5ds.sample((x, y) for x, y in zip(points.x, points.y))\n",
    ")) / 10000\n",
    "\n",
    "#\n",
    "# generate ndwi for sampled points\n",
    "#\n",
    "ndwi_values = ( band3_values - band5_values ) / ( band3_values + band5_values )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-jonathan",
   "metadata": {},
   "source": [
    "To finish this step, let's bind all extracted values in the same array. In this form, each point has values from `band 3`, `band 5` and `ndwi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# binding results\n",
    "#\n",
    "points = numpy.hstack((band3_values, band5_values, ndwi_values))\n",
    "\n",
    "points[0:5, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-bahrain",
   "metadata": {},
   "source": [
    "**Water Mask using linear separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-hungary",
   "metadata": {},
   "source": [
    "Create a linear separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier().fit(points, \n",
    "                            samples_ibge_filtered[\"class\"].astype(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-darwin",
   "metadata": {},
   "source": [
    "Generate the water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create data with bands 3, 5 and ndwi\n",
    "#\n",
    "band3 = item.read(\"band3\", bbox = bbox) / 10000\n",
    "band5 = item.read(\"band5\", bbox = bbox) / 10000\n",
    "\n",
    "ndwi  = ( band3 - band5 ) / ( band3 + band5 )\n",
    "\n",
    "#\n",
    "# bind all matrices\n",
    "#\n",
    "raster = numpy.stack( (band3, band5, ndwi) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# use raster to generate the water mask\n",
    "#\n",
    "prediction_array = model.predict(raster.T.reshape((-1, 3)))\n",
    "\n",
    "#\n",
    "# reshape to input raster dimensions\n",
    "#\n",
    "prediction_array = prediction_array.reshape(raster.shape[2], raster.shape[1]).T.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-settlement",
   "metadata": {},
   "source": [
    "Plot classified image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "plt.imshow(prediction_array, cmap='GnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-proposal",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = band3ds.profile\n",
    "profile[\"dtype\"] = \"int16\"\n",
    "profile[\"count\"] = 1\n",
    "\n",
    "with rasterio.open(\"water-mask-classification.tif\", \"w\", **profile) as file:\n",
    "    file.write(prediction_array[numpy.newaxis, ...].astype('int16'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
