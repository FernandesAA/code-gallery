{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-heading",
   "metadata": {},
   "source": [
    "<img src=\"../../../img/logo-bdc.png\" align=\"right\" width=\"64\"/>\n",
    "\n",
    "# <span style=\"color:#336699\">Labelling data points with WLTS</span>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "<div style=text-align: left;>\n",
    "    <a href=\"https://nbviewer.jupyter.org/github/brazil-data-cube/code-gallery/blob/master/jupyter/Python/wlts/wlts-introduction.ipynb\"><img src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" align=\"center\"/></a>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Fabiana Zioti<sup><a href=\"https://orcid.org/0000-0002-7305-6043\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Felipe Menino Carlos<sup><a href=\"https://orcid.org/0000-0002-3334-4315\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Karine Reis Ferreira<sup><a href=\"https://orcid.org/0000-0003-2656-5504\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Gilberto R. Queiroz<sup><a href=\"https://orcid.org/0000-0001-7534-0219\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    Earth Observation and Geoinformatics Division, National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:brazildatacube@inpe.br\">brazildatacube@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: March 24, 2021\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter Notebook shows how to use the WLTS API to label a set of data points considering two well known land use and land cover collections: Projeto de Mapeamento Anual da Cobertura e Uso do Solo no Brasil (MapBiomas) version 5 - Mapa de uso e cobertura da Terra and Instituto Brasileiro de Geografia e Estatística (IBGE) - Monitoramento e uso da Terra. The data points were created using a regular grid. After labelling these data points, the example illustrates a possible scenario for comparing the class agreement between the collections regarding these points. Finally, the data points with agreement in both collections are used to compute a water mask over an region of interest.\n",
    "</div>    \n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>This Jupyter Notebook is supplement to the following papers:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px; margin-top:10px\">\n",
    "      <p> Ferreira, K.R.; Queiroz, G.R.; Vinhas, L.; Marujo, R.F.B.; Simoes, R.E.O.; Picoli, M.C.A.; Camara, G.; Cartaxo, R.; Gomes, V.C.F.; Santos, L.A.; Sanchez, A.H.; Arcanjo, J.S.; Fronza, J.G.; Noronha, C.A.; Costa, R.W.; Zaglia, M.C.; Zioti, F.; Korting, T.S.; Soares, A.R.; Chaves, M.E.D.; Fonseca, L.M.G. 2020. Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sens. 12, no. 24: 4033. DOI: <a href=\"https://doi.org/10.3390/rs12244033\" target=\"_blank\">10.3390/rs12244033</a>. </p>\n",
    "      <p> Zioti, F.; Gomes, V.C.F.; Ferreira, K.R.; Queiroz, G.R.; Rodriguez, E. L. 2019. Um ambiente para acesso e análise de trajetórias de uso e cobertura da Terra. Anais do XIX Simpósio Brasileiro de Sensoriamento Remoto.São José dos Campos, INPE, 2019. <a href=\"https://proceedings.science/sbsr-2019/papers/um-ambiente-para-acesso-e-analise-de-trajetorias-de-uso-e-cobertura-da-terra\" target=\"_blank\"> Online </a>. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-livestock",
   "metadata": {},
   "source": [
    "# Study Area\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-turning",
   "metadata": {},
   "source": [
    "The study area is located in the Pará state, Brazil, in Amazon biome as depicted in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-engine",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"../../../img/wlts/example_wlts_area.png\" width=\"600\" />,\n",
    "    <br/>\n",
    "    <b>Figure 1</b> - Labelling data points with WLTS- Study Area.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-helicopter",
   "metadata": {},
   "source": [
    "# Python Client API\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-firewall",
   "metadata": {},
   "source": [
    "For running the examples in this Jupyter Notebook you will need to install the [WLTS client for Python](https://github.com/brazil-data-cube/wlts.py).To install it from GitHub using `pip`, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/brazil-data-cube/wlts.py@v0.4.0-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-mount",
   "metadata": {},
   "source": [
    "We also use the follow libraries: [numpy](https://numpy.org/), [rasterio](https://rasterio.readthedocs.io/en/latest/), [pandas](https://pandas.pydata.org/), [geopandas](https://geopandas.org/), [seaborn](https://seaborn.pydata.org/), [matplotlib](https://matplotlib.org/), [sklearn](https://scikit-learn.org/stable/), [stac.py] (https://github.com/brazil-data-cube/stac.py). To install those libraries from PyPI using pip, use the following commands:\n",
    "\n",
    "> pip install numpy rasterio pandas geopandas seaborn matplotlib sklearn folium stac.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-prevention",
   "metadata": {},
   "source": [
    "# Set the service and load the data points\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wlts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-broadcast",
   "metadata": {},
   "source": [
    "Define the service to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = wlts.WLTS('https://brazildatacube.dpi.inpe.br/wlts/')\n",
    "service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "service.collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-european",
   "metadata": {},
   "source": [
    "**Sampling GRID**\n",
    "\n",
    "To extract the samples, use will be made of a sampling grid with equally spaced locations. Below, the grid is loaded using the GeoPandas library.\n",
    "\n",
    ">  The sample points used below were generated using [QGIS GIS](https://qgis.org/pt_BR/site/). If you wish, you can use the [Verde] library (https://www.fatiando.org/verde/latest/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = geopandas.read_file(\"/vsicurl/https://brazildatacube.dpi.inpe.br/public/workshop/bdc-2020-03/wlts/samples-points/sample-points.shp\")\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-effort",
   "metadata": {},
   "source": [
    "Below, each grid point's spatial location is presented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# extract sample long, lat\n",
    "#\n",
    "latlon = samples_df.geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "\n",
    "#\n",
    "# create folium map\n",
    "#\n",
    "folium_map = folium.Map( location=[-0.52, -51.1526], zoom_start=12)\n",
    "\n",
    "#\n",
    "# Google Satellite Layer\n",
    "#\n",
    "tile = folium.TileLayer(\n",
    "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
    "        attr = 'Google',\n",
    "        name = 'Google Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(folium_map)\n",
    "\n",
    "#\n",
    "# add marker to map\n",
    "#\n",
    "for coord in latlon:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], fill_color='#43d9de', radius=3).add_to( folium_map )\n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-terminology",
   "metadata": {},
   "source": [
    "# Labelling the data points\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-projection",
   "metadata": {},
   "source": [
    "We will use the grid points' location to extract classes defined by different mapping projects. In this way, each point will be associated with a class.\n",
    "\n",
    "> In WLTS, the data from each of the projects is represented through collections. A collection is an aggregation of data from different years of the same project.\n",
    "\n",
    "In this example scenario, we will perform a concordance analysis between each project to choose the best samples. This analysis provides a set of samples based on the knowledge applied by each project for the choice of classes.\n",
    "\n",
    "> Note that this is an example scenario. The complexities of a real scenario are not considered here. Therefore, problems related to Spatio-temporal differences between samples or their creation methodology are not considered!\n",
    "\n",
    "The sample labels will be extracted separately in the subsections to facilitate their application in the example that will be created, but the [wlts.py library](https://github.com/brazil-data-cube/wlts.py/) supports the extraction of data considering multiple projects at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-sucking",
   "metadata": {},
   "source": [
    "**IBGE - Monitoramento e uso da Terra (2018)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-seeking",
   "metadata": {},
   "source": [
    "In WLTS, the collection with IBGE data from the Land Use Monitoring project is in the collection named `ibge_land_use_cover`. The code below extracts the label of this collection in the year 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge = []\n",
    "\n",
    "#\n",
    "# Extract classes with WLTS\n",
    "#\n",
    "for point_row in samples_df.iterrows():\n",
    "    point_row = point_row[1]\n",
    "    \n",
    "    ibge_class = service.tj(latitude  = point_row.geometry.y, \n",
    "                            longitude = point_row.geometry.x, \n",
    "                            start_date = 2018, end_date = 2018,\n",
    "                            collections = \"ibge_cobertura_uso_terra\")\n",
    "    \n",
    "    samples_ibge.append(ibge_class.df())\n",
    "\n",
    "#\n",
    "# Create a Data Frame\n",
    "#\n",
    "samples_ibge = pandas.concat(samples_ibge).reset_index(drop=True)\n",
    "samples_ibge[\"geometry\"] = samples_df[\"geometry\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-pizza",
   "metadata": {},
   "source": [
    "The table below presents the classes, with only one year, extracted for all the grid points presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-raise",
   "metadata": {},
   "source": [
    "**MapBiomas version 5 - Mapa de uso e cobertura da Terra**\n",
    "\n",
    "Analogous to the IBGE data, this section extracts the data from MapBiomas. In WLTS, the data from MapBiomas (Version 5) are represented through the collection `mapbiomas5_amazonia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas = []\n",
    "\n",
    "#\n",
    "# Extract classes with WLTS\n",
    "#\n",
    "for point_row in samples_df.iterrows():\n",
    "    point_row = point_row[1]\n",
    "    \n",
    "    mapbiomas_class = service.tj(latitude  = point_row.geometry.y, \n",
    "                                 longitude = point_row.geometry.x, \n",
    "                                 start_date = 2018,\n",
    "                                 end_date = 2018,\n",
    "                                 collections = \"mapbiomas5_amazonia\")\n",
    "    \n",
    "    samples_mapbiomas.append(mapbiomas_class.df())\n",
    "\n",
    "#\n",
    "# Create a Data Frame\n",
    "#\n",
    "samples_mapbiomas = pandas.concat(samples_mapbiomas).reset_index(drop=True)\n",
    "samples_mapbiomas[\"geometry\"] = samples_df[\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-miller",
   "metadata": {},
   "source": [
    "# Prepare data to a concordance analysis\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-editor",
   "metadata": {},
   "source": [
    "This section prepares the data for the concordance analysis. In this process, all points identified as water have their path values converted to `1`, while all other values are represented by `0`.\n",
    "\n",
    "This conversion is applied considering that there is one class that represents the Water element for each collection. The table below summarizes how each collection does this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-employee",
   "metadata": {},
   "source": [
    "|         Collection        \t|      Nomenclature for water class   \t|\n",
    "|:-------------------------:\t|:----------------------------------:\t|\n",
    "|        IBGE (2018)        \t|      Corpo d'água Continental      \t|\n",
    "| MapBiomas Versão 5 (2018) \t|         Rio, Lago e Oceano         \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-costa",
   "metadata": {},
   "source": [
    "Considering the information in the table, below each of the collections is prepared for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-ability",
   "metadata": {},
   "source": [
    "`IBGE Collection (2018)`\n",
    "\n",
    "> After running the command below, notice that the `class` column has its value summed to the values `0` and `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.loc[samples_ibge[\"class\"] != \"Corpo d'água Continental\", \"class\"] = 0\n",
    "samples_ibge.loc[samples_ibge[\"class\"] == \"Corpo d'água Continental\", \"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-soundtrack",
   "metadata": {},
   "source": [
    "`MaBiomas Collection (2018)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] != \"Rio, Lago e Oceano\", \"class\"] = 0\n",
    "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] == \"Rio, Lago e Oceano\", \"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mapbiomas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-morocco",
   "metadata": {},
   "source": [
    "# Concordance analysis\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-booth",
   "metadata": {},
   "source": [
    "Below we will generate an example of a concordance analysis. A confusion matrix is generated to visualize and quantify the points that have a concordance. After visualizing the matrix, the data is filtered so that only the points where there is concordance are considered.\n",
    "\n",
    "> **Note**: The analysis below does not consider many of the practical complexities involved in this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate the confusion matrix\n",
    "#\n",
    "\n",
    "cm_arr = confusion_matrix(samples_ibge[\"class\"].astype(\"int\"), samples_mapbiomas[\"class\"].astype(\"int\"))\n",
    "\n",
    "#\n",
    "# formating results\n",
    "#\n",
    "reference = [\"Non-Water\", \"Water\"]\n",
    "\n",
    "cm_pd = pandas.DataFrame(cm_arr, columns = reference, index = reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 100)\n",
    "\n",
    "#\n",
    "# plot matrix\n",
    "#\n",
    "seaborn.heatmap(cm_pd, annot=True, fmt = 'g', cmap=\"YlGnBu\", cbar = False)\n",
    "\n",
    "#\n",
    "# configure labels\n",
    "#\n",
    "plt.title(\"Concordance matrix\")\n",
    "plt.ylabel(\"IBGE (2018)\")\n",
    "plt.xlabel(\"MapBiomas 5 (2018)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-signature",
   "metadata": {},
   "source": [
    "> Below, the samples are filtered considering the equality between both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# generate the \"true matrix\" based on classes matching\n",
    "#\n",
    "true_matrix = samples_ibge[\"class\"].values == samples_mapbiomas[\"class\"].values\n",
    "\n",
    "#\n",
    "# filtering data\n",
    "#\n",
    "samples_ibge_filtered = samples_ibge[true_matrix]\n",
    "samples_mapbiomas_filtered  = samples_mapbiomas[true_matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ibge_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-amendment",
   "metadata": {},
   "source": [
    "**Visualizing the filtered points in the geographical space**\n",
    "\n",
    "The map below shows the filtered samples. The `blue` samples represent the concordant elements. On the other hand, the `yellow` ones are the points where the ready did not agree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# extract sample long, lat\n",
    "#\n",
    "latlon = samples_ibge_filtered.geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "latlon_not_valid = samples_ibge[~true_matrix].geometry.apply(lambda p: (p.y, p.x)).tolist()\n",
    "\n",
    "#\n",
    "# create folium map\n",
    "#\n",
    "folium_map = folium.Map( location=[-0.52, -51.1526], zoom_start=12)\n",
    "\n",
    "#\n",
    "# Google Satellite Layer\n",
    "#\n",
    "tile = folium.TileLayer(\n",
    "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
    "        attr = 'Google',\n",
    "        name = 'Google Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(folium_map)\n",
    "\n",
    "#\n",
    "# add marker to map (concordance samples)\n",
    "#\n",
    "for coord in latlon:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], fill_color='#43d9de', radius=9).add_to( folium_map )\n",
    "\n",
    "#\n",
    "# add marker to map (not concordante samples)\n",
    "#\n",
    "for coord in latlon_not_valid:\n",
    "    folium.CircleMarker( location=[ coord[0], coord[1] ], color='#F5AD46', radius=9).add_to( folium_map )\n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-hopkins",
   "metadata": {},
   "source": [
    "# Example: Computing Water Mask for a single image\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-surgery",
   "metadata": {},
   "source": [
    "In this section, the previously extracted and filtered samples will be used for training a linear classifier. After training, the model is applied. The classification process will be done considering a scene extracted from the Landsat-8/OLI data cube (temporal composition of 16 days and the pixel choice with less cloud influence done through the STACK algorithm).\n",
    "\n",
    "The defined study region is located within the Amazon biome, in a cube tile in Pará.\n",
    "\n",
    "> In this example, to reduce the computational requirements, a small region of the scene will be used, this one intersecting with the location of the grid points presented earlier. Furthermore, to facilitate classification, the **N**ormalized **D**ifference **W**ater **I**ndex (NDWI) is calculated.\n",
    "\n",
    "For the acquisition of the Landsat-8/OLI scene the [stac.py](https://github.com/brazil-data-cube/stac.py) client will be used. Below, the client is imported and the credentials are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = stac.STAC('https://brazildatacube.dpi.inpe.br/stac/', access_token='change-me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# defining roi bbox\n",
    "#\n",
    "bbox = (-51.232048, \n",
    "        -0.594217, \n",
    "        -51.078365, \n",
    "        -0.464596)\n",
    "\n",
    "#\n",
    "# query stac!\n",
    "#\n",
    "items = service.search({\n",
    "    'limit': 1,\n",
    "    'bbox': bbox,\n",
    "    'datetime': '2018-06-10/2018-06-25',\n",
    "    'collections': ['LC8_30_16D_STK-1']\n",
    "})\n",
    "\n",
    "#\n",
    "# visualizing the result (must be have scene)\n",
    "#\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# select item\n",
    "#\n",
    "item = items.features[0]\n",
    "\n",
    "#\n",
    "# load bands to generate the rgb\n",
    "#\n",
    "\n",
    "band7 = item.read(\"band7\", bbox = bbox)\n",
    "band5 = item.read(\"band5\", bbox = bbox)\n",
    "band4 = item.read(\"band4\", bbox = bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-accreditation",
   "metadata": {},
   "source": [
    "The code above, shows the scene loaded in grid region. In this example, the RGB visualization uses the `band 7`, `band 5` and `band 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# defining the rgb matrix\n",
    "#\n",
    "rgb = numpy.stack((band7, band5, band4))\n",
    "\n",
    "#\n",
    "# create a figure\n",
    "#\n",
    "plt.figure(dpi = 120)\n",
    "\n",
    "#\n",
    "# plot!\n",
    "#\n",
    "show(rgb / 10000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-stress",
   "metadata": {},
   "source": [
    "**Calculating the Normalized Difference Water Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-heart",
   "metadata": {},
   "source": [
    "Loading images using [rasterio](https://rasterio.readthedocs.io/en/latest/). To do this, the url of each scene, returned in stac query is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "band3ds = rasterio.open(item[\"assets\"][\"band3\"][\"href\"])\n",
    "band5ds = rasterio.open(item[\"assets\"][\"band5\"][\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-webster",
   "metadata": {},
   "source": [
    "To extract values, we need reproject grid data point to raster **C**oordinate **R**eference **S**ystem (CRS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# reprojects grid points\n",
    "#\n",
    "samples_ibge_filtered = geopandas.GeoDataFrame(samples_ibge_filtered)\\\n",
    "                                .set_geometry(\"geometry\")\\\n",
    "                                .set_crs(\"EPSG:4326\")\n",
    "\n",
    "points = samples_ibge_filtered[\"geometry\"].to_crs(band3ds.crs)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tumor",
   "metadata": {},
   "source": [
    "now, we can extract spectral data (Bands 3 and 5) for each data points filterd above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extract values for band 3\n",
    "#\n",
    "band3_values = numpy.array(list(\n",
    "    band3ds.sample((x, y) for x, y in zip(points.x, points.y))\n",
    "))  / 10000\n",
    "\n",
    "#\n",
    "# Extract values for band 5\n",
    "#\n",
    "band5_values = numpy.array(list(\n",
    "    band5ds.sample((x, y) for x, y in zip(points.x, points.y))\n",
    ")) / 10000\n",
    "\n",
    "#\n",
    "# generate ndwi for sampled points\n",
    "#\n",
    "ndwi_values = ( band3_values - band5_values ) / ( band3_values + band5_values )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-courage",
   "metadata": {},
   "source": [
    "To finish this step, let's bind all extracted values in the same array. In this form, each point has values from `band 3`, `band 5` and `ndwi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# binding results\n",
    "#\n",
    "points = numpy.hstack((band3_values, band5_values, ndwi_values))\n",
    "\n",
    "points[0:5, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-utility",
   "metadata": {},
   "source": [
    "**Water Mask using linear separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-transportation",
   "metadata": {},
   "source": [
    "Create a linear separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier().fit(points, \n",
    "                            samples_ibge_filtered[\"class\"].astype(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-monday",
   "metadata": {},
   "source": [
    "Generate the water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create data with bands 3, 5 and ndwi\n",
    "#\n",
    "band3 = item.read(\"band3\", bbox = bbox) / 10000\n",
    "band5 = item.read(\"band5\", bbox = bbox) / 10000\n",
    "\n",
    "ndwi  = ( band3 - band5 ) / ( band3 + band5 )\n",
    "\n",
    "#\n",
    "# bind all matrices\n",
    "#\n",
    "raster = numpy.stack( (band3, band5, ndwi) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# use raster to generate the water mask\n",
    "#\n",
    "prediction_array = model.predict(raster.T.reshape((-1, 3)))\n",
    "\n",
    "#\n",
    "# reshape to input raster dimensions\n",
    "#\n",
    "prediction_array = prediction_array.reshape(raster.shape[2], raster.shape[1]).T.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-jason",
   "metadata": {},
   "source": [
    "Plot classified image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "plt.imshow(prediction_array, cmap='GnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-devices",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = band3ds.profile\n",
    "profile[\"dtype\"] = \"int16\"\n",
    "profile[\"count\"] = 1\n",
    "\n",
    "with rasterio.open(\"water-mask-classification.tif\", \"w\", **profile) as file:\n",
    "    file.write(prediction_array[numpy.newaxis, ...].astype('int16'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
